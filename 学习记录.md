# 23-CS-谭博涵

## Gym-Taxi游戏基础模型

[model_1](model_1.py)

### 环境构建

​	我使用gym库自带的Taxi-v3作为基础环境。

​	考虑到这个状态空间包括了出租车的位置(有25种可能性,因为环境假设了一个5x5的网格世界)、乘客的位置(有4种可能性,分别对应Red、Green、Yellow和Blue四个位置)、目的地(也有4种可能性,同样对应四个指定位置)，100个状态是乘客的位置刚好是目的地的位置,这表示这一轮游戏的结束。故环境的state有500个，而每次操作有上，下，左，右，接乘客，放乘客6种操作，故action有6个。Q表设置为**500*6**大小。

### RL算法的选择

​	**Q-learning：**使用下一状态最优值来更新当前状态Q值：
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*\max(Q(s',a'))-Q(s,a))
$$
​		其中，$\alpha$为学习率，$R$为reward，$\gamma$为折扣因子。

​	**Sarsa：**使用下一状态选择的动作来更新当前状态Q值。
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*Q(s',a')-Q(s,a))
$$
​	二者区别：在Sarsa算法中，Q值的更新取决于当前状态采取的动作和新状态下选择的下一个动作；在Q-learning算法中，Q值的更新取决于当前状态下采取的动作，而下一动作是基于最大Q值选取的。这意味着Sarsa算法更考虑**当前动作**，Q-learning算法更考虑**最大reward**。

​	所以，Sarsa训练的模型更加稳定，Q-learning算法训练的模型回报更加可观。

### 训练和测试

​	二者的行走方式一致，区别在于train需要learn来更新Q表，test只需根据Q表来选择行走方式。

## Gym-Taxi游戏参数改进

### train_episode的选择

​	在Gym-Taxi环境中，由于离散情况不多，故经过适当的训练后，即可达到预期效果，若train_episode过大，大多次训练进行的是重复操作，训练成本过大，若train_episode过小，则无法对某些情况作出合理的判断选择，不能将更加正确的抉择的Q值凸显出来。

​	在本次训练中，我选择的train_episode为5000。

![](.\level-1\train.png)

![test](.\level-1\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.3$，train_episode=5000，test_episode=5000.

### $\epsilon$的选择

​	$\epsilon$表示智能体在选择action时，随机选择一个动作的概率，当其过高时，智能体不能很好的运用经验回放进行操作训练，当其过低时，不能完成多情况选择时的探索。

​	在本次训练中，我选择$\epsilon=0.01$.

![train](.\level-2\train.png)

![train](.\level-2\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.

### learning-rate的选择

​	learning-rate（以下简记为lr）作为学习率，指动作后的reward对上一状态Q值的更新权重。

适当的lr可以使模型快速收敛到合适的reward，过高的lr会使模型在reward谷底来回振荡，过低的lr会使训练成本——训练回合增大。

​	在本次训练中，我选择lr=0.1。

​	测试结果同上一小节。

### $\gamma$的选择

​	$\gamma$即对未来reward的衰减值。$\gamma$越接近1，机器对未来的reward越敏感。我们可以用股票的方式来理解，10年后股票的涨跌对当下的买进和买入其实影响不大。我们引入衰减率即可表示出这种影响：
$$
\Delta reward=\Sigma_{k=0}^{n}\gamma^{k}reward_k
$$
​	在本次训练中，n=1，即只考虑当前和下一状态。

​	$\gamma$过高，则较远未来影响衰减体现不出来，$\gamma$过低，则较近未来影响体现不出来。

​	在本次训练中，我选择$\gamma=0.97$。

​	测试结果同上上小节。

## Gym-Taxi游戏环境改进

​	由于自己能力有限，对gym自带的taxi环境难以操作更改，故自己创建了一个taxi环境：[model_2](model_2.py).

![train](.\level-3\train.png)

![test](.\level-3\test.png)

### 更新reward表

​	首先,对于gym自带的taxi环境的reward,只有在街道乘客和正确放下乘客才能得到奖励,在此之前,智能体更多的是效率低下的随机探索。即像动态规划一样，在探索到最终状态才能逐步往前迭代，不能很好的利用整个环境。

​	由此，我学习了Reward Shaping技巧，使用曼哈顿距离作为Reward Shaping公式，在接取乘客之前，以距乘客的曼哈顿距离构造梯度reward表，在接取乘客之后，以距目的地的曼哈顿距离构造reward表，从而使智能体更好的与环境交互。

​	具体修改如下：**撞墙、错误接放：**-50；**向目标前进：**10-曼哈顿距离；**向目标远离：**曼哈顿距离-11；**正确接放**：20。

​	按以上处理后，发现智能体会在5-2和5-3反复行动，认为是仅操作终点的曼哈顿距离不能很好的体现非下一步墙体的reward。注意到，当终点为1-1或5-1时，智能体必经过3-1，而到3-1不会出现导致反复行动，同理，当终点是1-5或5-4时，必（假设）经过3-4。

​	所以我们加入优化，将3-1和3-4分别作为中间点，运用曼哈顿分步奖励，让智能体分步行动。

​	训练效果如下：

![train](.\level-4\train.png)

![test](.\level-4\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.	

​	考虑到reward shaping所设置的reward普遍较原reward较大，进行适当缩放，我们可以看到，利用reward shaping技巧可以很大程度减小前期的探索步骤，收敛过程亦相当可观。

​	值得注意的是，利用reward shaping test时可以很好的处理未进行train的情况，即可以更好的实时交互。

​	需要注意的是，对于当下Gym-Taxi游戏，我们是可以直接手动完成路线规划的，即设置大量的中间点，会使路线更加的美观和优秀甚至统一，但这样并不适用于大型离散或连续型的环境，也体现不了强化学习与环境的交互。

​	

## Gym-Pong游戏

### DQN神经网络构建

​	对比Gym-Taxi的离散情况，Gym-Pong环境是RGB图像，属于半连续状态，不能像Gym-Taxi一样构建Q-table进行更新。但注意到Q-table存在的意义是指导agent的行动，给出每个行动的reward，进而做出决策。对于连续情况，我们需要的本质是寻找一个东西去指导agent做选择，即神经网络。

​	神经网络可以将大数据处理成向量形式（在Pong游戏中为图像），在经过 神经网络后输出对应的reward，用以指导agent的action。同时我们可以像更新Q-table一样更新神经网络的参数，将神经网络训练成类似Q-table的函数。

​	在Pong环境中，我将一帧图片处理成shape=80*80的像素阵，压平成size=6400的向量，传入3层神经网络，得到size=6的rewards。

### 经验回放

​	对于Q-table，每个状态都有一个独立的空间存储reward，在除更新时一般情况不用考虑其他状态影响它的reward，但对于连续情况，所有状态共享一个神经网络，其他状态对神经网络的更新会影响到之前更新的权重，在循环往复的过程中，较难达到预期效果。

​	对于这种时序上引起的反向更新，我们引入**经验回放**，即将过往状态存入一个经验池，在未来更新中不断采用先前的经验更新，同时，随着网络的更新，经验池溢出时也会沿着时序更新，让网络总体上趋于最优。

​	对于一个经验池，我们使用队列构造（利用其先进先出的优势），每一次更新，我们从经验池中随机选取若干个经验用于更新网络。

### 策略梯度算法

​	在DQN神经网络中，在选择action时，对于网络输出的actions我们几乎是直接选取了价值最大的action，哪怕一个动作只是必=比另一个动作预期稍大。这样的选择会使动作僵化，导致那些潜在预期比较大的动作被采样的频率不高，更新速度较慢。

​	若我们选取action时，是使用rewards值所构成的归一化概率来选取action，则很好的解决了这个问题，即使用softmax函数对最后的rewards处理，它的输出即采取每个动作的概率，显然它是渐变的。

​	



























