# 23-CS-谭博涵

## RL基础知识

### RL构件和基础

​	强化学习通俗上智能体通过与环境交互，进行决策，积累奖励经验，不断更新决策方式达到训练目的。

​	通常，一个强化学习模型由agent、Q-table、environment、state、action、reward等基础构件组成，agent识别environment，根据Q-table做出决策action，执行action后得到environment反馈的reward，更新Q-table。

​	为什么需要强化学习呢？在算法竞赛中，有一种贪心算法，即每次选取最优反馈的策略，最终可能得到最优结果。但在实际情况中，很多环境并不适用于这种算法，就比如你去贷款，当下会有收益，但未来你仍需偿还，故当下贷款的收益实际需要长远的目光。那么Q-table记录了这种目光并用于agent的决策。

### RL算法的选择

​	如何更新Q表呢？动态规划是一种很好的方式，不同于算法竞赛中的动态规划，在这里，我们指将最终结束的情况一步步回传，不断向前更新Q-table。[具体演示实验视频](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_td.html)。

​	我们这种回传更新的方式有两种，Q-learning和Sarsa：

​	**Q-learning：**使用下一状态最优值来更新当前状态Q值：
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*\max(Q(s',a'))-Q(s,a))
$$
​		其中，$\alpha$为学习率，$R$为reward，$\gamma$为折扣因子。

![Q-learning](src\3.png)

​	**Sarsa：**使用下一状态选择的动作来更新当前状态Q值。
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*Q(s',a')-Q(s,a))
$$
​	二者区别：在Sarsa算法中，Q值的更新取决于当前状态采取的动作和新状态下选择的下一个动作，照顾到每一个动作；在Q-learning算法中，Q值的更新取决于当前状态下采取的动作，而下一动作是基于最大Q值选取的，更加冒险。这意味着Sarsa算法更考虑**当前动作**，Q-learning算法更考虑**最大reward**。

![Sarsa](src\2.png)

​	![1](src\1.png)

​	由图中可以看到，Q-learning只需270回合，Sarsa需要550回合才能找到更优路径，而对于之后的reward，Sarsa更加稳定。故，Sarsa训练的模型更加稳定，Q-learning算法训练的模型回报更加可观。





## Gym-Taxi游戏基础模型

[model_1](model_1.py)

### 环境构建

​	我使用gym库自带的Taxi-v3作为基础环境。

![4](src\4.png)

​	考虑到这个状态空间包括了出租车的位置(有25种可能性,因为环境假设了一个5x5的网格世界)、乘客的位置(有4种可能性,分别对应Red、Green、Yellow和Blue四个位置)、目的地(也有4种可能性,同样对应四个指定位置)，100个状态是乘客的位置刚好是目的地的位置,这表示这一轮游戏的结束。故环境的state有500个，而每次操作有上，下，左，右，接乘客，放乘客6种操作，故action有6个。Q表设置为**500*6**大小。

### 训练和测试

​	我使用Q-learning算法进行训练。

​	二者的行走方式一致，区别在于train需要learn来更新Q表，test只需根据Q表来选择行走方式。

## Gym-Taxi游戏参数改进

### train_episode的选择

​	在Gym-Taxi环境中，由于离散情况不多，故经过适当的训练后，即可达到预期效果，若train_episode过大，大多次训练进行的是重复操作，训练成本过大，若train_episode过小，则无法对某些情况作出合理的判断选择，不能将更加正确的抉择的Q值凸显出来。

​	在本次训练中，我选择的train_episode为5000。

![](.\level-1\train.png)

![test](.\level-1\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.3$，train_episode=5000，test_episode=5000.

### $\epsilon$的选择

​	$\epsilon$表示智能体在选择action时，随机选择一个动作的概率，当其过高时，智能体不能很好的运用经验回放进行操作训练，当其过低时，不能完成多情况选择时的探索。

​	在本次训练中，我选择$\epsilon=0.01$.

![train](.\level-2\train.png)

![train](.\level-2\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.

### learning-rate的选择

​	learning-rate（以下简记为lr）作为学习率，指动作后的reward对上一状态Q值的更新权重。

​	适当的lr可以使模型快速收敛到合适的reward，过高的lr会使模型在reward谷底来回振荡；过低的lr会使训练成本——训练回合增大，也会使模型在局部最优点无法跳出。

​	在本次训练中，我选择lr=0.1。

​	测试结果同上一小节。

### $\gamma$的选择

​	$\gamma$即对未来reward的衰减值。$\gamma$越接近1，机器对未来的reward越敏感。我们可以用股票的方式来理解，10年后股票的涨跌对当下的买进和买入其实影响不大。我们引入衰减率即可表示出这种影响：
$$
\Delta reward=\Sigma_{k=0}^{n}\gamma^{k}reward_k
$$
​	在本次训练中，n=1，即只考虑当前和下一状态（时序差分）。

​	$\gamma$过高，则较远未来影响衰减体现不出来，$\gamma$过低，则较近未来影响体现不出来。

​	在本次训练中，我选择$\gamma=0.97$。

​	测试结果同上上小节。

## Gym-Taxi游戏环境改进

​	由于自己能力有限，对gym自带的taxi环境难以操作更改，故自己创建了一个taxi环境：[model_2](model_2.py).

![train](.\level-3\train.png)

![test](.\level-3\test.png)

### 更新reward表

​	首先,对于gym自带的taxi环境的reward,只有在街道乘客和正确放下乘客才能得到奖励,在此之前,智能体更多的是效率低下的随机探索。即像动态规划一样，在探索到最终状态才能逐步往前迭代，不能很好的利用整个环境。

​	由此，我学习了Reward Shaping技巧，使用曼哈顿距离作为Reward Shaping公式，在接取乘客之前，以距乘客的曼哈顿距离构造梯度reward表，在接取乘客之后，以距目的地的曼哈顿距离构造reward表，从而使智能体更好的与环境交互。

​	具体修改如下：**撞墙、错误接放：**-50；**向目标前进：**10-曼哈顿距离；**向目标远离：**曼哈顿距离-11；**正确接放**：20。

![5](src\5.png)

​	按以上处理后，发现智能体会在5-2和5-3反复行动，认为是仅操作终点的曼哈顿距离不能很好的体现非下一步墙体的reward。注意到，当终点为1-1或5-1时，智能体必经过3-1，而到3-1不会出现导致反复行动，同理，当终点是1-5或5-4时，必（假设）经过3-4。

​	所以我们加入优化，将3-1和3-4分别作为中间点，运用曼哈顿分步奖励，让智能体分步行动。

​	训练效果如下：

![train](.\level-4\train.png)

![test](.\level-4\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.	

​	考虑到reward shaping所设置的reward普遍较原reward较大，进行适当缩放，我们可以看到，利用reward shaping技巧可以很大程度减小前期的探索步骤，收敛过程亦相当可观。

​	值得注意的是，利用reward shaping test时可以很好的处理未进行train的情况，即可以更好的实时交互。

​	需要注意的是，对于当下Gym-Taxi游戏，我们是可以直接手动完成路线规划的，即设置大量的中间点，会使路线更加的美观和优秀甚至统一，但这样并不适用于大型离散或连续型的环境，也体现不了强化学习与环境的交互。

​	

## Gym-Pong游戏

-![6](src\6.gif)

### DQN神经网络构建

​	对比Gym-Taxi的离散情况，Gym-Pong环境是RGB图像，属于半连续状态，不能像Gym-Taxi一样构建Q-table进行更新。但注意到Q-table存在的意义是指导agent的行动，给出每个行动的reward，进而做出决策。对于连续情况，我们需要的本质是寻找一个东西去指导agent做选择，即神经网络。

​	神经网络可以将大数据处理成向量形式（在Pong游戏中为图像），在经过 神经网络后输出对应的reward，用以指导agent的action。同时我们可以像更新Q-table一样更新神经网络的参数，将神经网络训练成类似Q-table的函数。

​	在Pong环境中，我将一帧图片处理成shape=80*80的像素阵，压平成size=6400的向量，传入3层神经网络，得到size=6的rewards。

### 经验回放

​	对于Q-table，每个状态都有一个独立的空间存储reward，在除更新时一般情况不用考虑其他状态影响它的reward，但对于连续情况，所有状态共享一个神经网络，其他状态对神经网络的更新会影响到之前更新的权重。

​	在训练过程中，同一小段时间的图像大致一样，会导致神经网络朝着这个图像大幅更新，抵消之前的网络参数更新。对于这种时序上引起的反向更新，我们引入**经验回放**，即将过往状态存入一个经验池，在未来更新中不断采用先前的经验更新，同时，随着网络的更新，经验池溢出时也会使经验沿着时序更新，让网络总体上趋于最优。

​	对于一个经验池，我们使用队列构造（利用其先进先出的优势），每一次更新，我们从经验池中随机选取若干个经验用于更新网络。

### 网络更新优化

​	对于Q-table，我们选取下一Q_max_value时一般情况下也是无需考虑状态之间的影响，但对于复杂的DQN网络，状态数过多，神经网络评估的下一状态的值时很不稳定的，一个参数的更改可能会导致对其他状态的评估改变较大，比如在之前是-5，网络更新一次后变成12，就会导致我们对当前状态的更新出现误差，学习过程上下波动。

​	为了避免出现这种波动的情况，我们可以定义一个target_net，其是一个3000步不变的网络，用以评估下一状态的价值。

​	于是我们有了两个网络，一个是学习网络，用以计算当前状态的价值，不断更新；另一个是目标网络，每隔3000步更新（使用学习网络更新），用以评估下一状态的价值。

### 图像处理

​	游戏环境是一个RGB图像，但对于智能体而言，颜色的区分在gym-Pong中更多是阻碍其识别，我们将图像的有效区域切分后转为二值图像，即球和版为黑，其余部分为白。

![7](src\7.png)

### 卷积神经网络和池化

​	卷积神经网络是利用卷积核提取图像特征的一种方式，可以放大所需特征，对于DQN大型网络参数优化有明显效应。

​	在对图像进行卷积之后，加入池化层，以减少特征图的尺寸并保留重要特征。

![8](src\8.png)

DQN代码见[DQN](DQN_net.py).

## 优化算法

### 策略梯度算法

​	在DQN神经网络中，在选择action时，对于网络输出的actions我们几乎是直接选取了价值最大的action，哪怕一个动作只是比另一个动作预期稍大。这样的选择会使动作僵化，导致那些潜在预期比较大的动作被采样的频率不高，更新速度较慢或是难以达到预期，在使用前面的方法训练后，最优效果大致在21：6，不能很好的收敛。

​	若我们选取action时，是使用rewards值所构成的归一化概率来选取action，则很好的解决了这个问题，即使用softmax函数对最后的rewards处理，它的输出即采取每个动作的概率（使用SoftMax），显然它是渐变的。

​	当然，仅这样优化，既不能用off-policy的经验回放，也不能体现on-policy的优势。我们将每一局游戏当作一个训练周期，对这一整条策略进行梯度下降找到最优，即：
$$
\begin{aligned}
\nabla R_\theta &= \Sigma R(\tau)\nabla p_{\theta}(\tau)\\&=\Sigma R(\tau)p_{\theta}(\tau)\frac{\nabla p_{\theta}(\tau)}{p_{\theta}(\tau)}\\&=\Sigma R(\tau)p_{\theta}(\tau)\nabla \log p_{\theta}(\tau)\\&=E_{r\sim p_\theta(\tau)[R(\tau)\nabla\log p_\theta(\tau)]}
\end{aligned}
$$
​	为什么要将$p_\theta$的梯度转为$\log p_\theta$呢？

​	第一：我们通过图像了解到，y=x图像及其平滑，而对于y=log x这个凸函数，在采样分布不同时可以很好的拟合这种情况。

​	第二：我们在变换中又凑出了$\Sigma R(\tau)p(\tau)$，可以将其转化为期望的形式。

​	第三：我们注意到$p_\theta(\tau)=p(s_0)\Pi_{t=0}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)$为乘积形式，对$\theta$ 的求导并不好操作，将其套上log后我们可以得到和式，即：
$$
\begin{aligned}
\nabla \log\pi_\theta(\tau)&=\nabla \log[p(s_0)\Pi_{t=0}^T\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t)]\\&=\nabla[\log p(s_0)+\Sigma_{t=0}^T\log \pi_\theta(a_t|s_t)+\Sigma_{t=0}^T\log p(s_{t+1}|s_t,a_t)]\\&=\Sigma_{t=0}^T\nabla_\theta\log\pi_\theta(a_t|s_t)
\end{aligned}
$$
​	因此：
$$
\begin{aligned}
\nabla_\theta J(\theta)&=E_{\tau\sim\pi_\theta(\tau)}[\Sigma_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_{i,t}|s_{i,t})\Sigma_{t=0}^{T}r(s_{i,t},a_{i,t})]\\&=\frac{1}{N}\Sigma_{i=1}^{N}[\Sigma_{t=0}^{T}\nabla_\theta\log\pi_\theta(a_{i,t}|s_{i,t})\Sigma_{t=0}^{T}r(s_{i,t},a_{i,t})]
\end{aligned}
$$
​	这就是策略梯度的优化公式，更进一步，可以加入偏移量等优化形式，为了简便，本次训练未加入。

![9](src\9.png)

### 奖励衰减机制

​	在前文$\gamma$的选择中，我们提到了如下公式：
$$
\Delta reward=\Sigma_{k=0}^{n}\gamma^{k}reward_k
$$
​	在使用策略梯度算法时，一般是传入连续的多帧图像（本次训练以一局游戏为一组，即蒙特卡洛方法），这些奖励是连续的，不像经验池在短时间内只能得到下一状态的reward，我们就可以使用多步的奖励衰减去更新reward，将正负奖励延申，减少不必要的随机探索，使网络更快的收敛，体现了on-policy的特殊优越性

策略梯度代码见[p_net](p_net.py).

### 演员-评论家算法

​	在使用DQN网络时，我们基于reward进行选择动作，弊端已经有所提及。而策略梯度网络也有其对应的弊端，它是通过累计奖励来更新网络的，在得到终止状态，才拿到最终奖励，很显然，当最终状态很远时，我们对策略的估计难免有所偏差，总的来说，仅用时序差分reward对训练网络还有待改进。

​	究其原因，是时序差分对每个状态的未来收获估计不准确导致的，我们可以利用神经网络估计当前状态的未来收益期望，判断每个状态或者每个状态-动作对的价值。在本次学习中我使用状态价值函数：
$$
V = E(s)
$$
​	优势函数：
$$
A = Reward - V
$$
​	最大化优势函数即可，当然，这里的reward亦可以使用一个神经网络Q来近似。即标准优势函数：
$$
A^\pi(s,a) = Q^\pi(s,a)-V^\pi(s)
$$
​	这里的Q函数我没有使用网络估计，而是沿用DQN的TD。

演员-评论家算法见[actor_critic_net.py](actor_critic_net.py).

### A2C算法

​	在上一节提到了优势函数，其推导出来的训练函数如下：
$$
\nabla R(\tau)=\frac{1}{N}\Sigma_{n=1}^{N}\Sigma_{t=1}^{T_n}(Q^{\pi_{\theta}}(s_{t}^{n},a_{t}^{n})-V^{\pi_{\theta}}(s_{t}^{n}))\nabla\log p_{\theta}(a_t^n|s_t^n)
$$
​	即对于一整条策略进行优势值累计，并最大化训练函数。从公式中可以看出，我们要完成两种估计，那么直观思路就是我们需要两个网络：Q-network 和 V-network，如果这样做估测不准的风险就变成两倍。所以我们何不只估测一个网络？

​	事实上在这个 Actor-Critic 方法里面，我们可以只估测 V 这个网络，并且用 V 的值来表示 Q 的值，即：
$$
Q^{\pi_{\theta}}(s_{t}^{n},a_{t}^{n})=r_t^n+\gamma V^{\pi_{\theta}}(s_{t+1}^{n})
$$
​	用该式子将对训练函数进行化简，即：
$$
\nabla R(\tau)=\frac{1}{N}\Sigma_{n=1}^{N}\Sigma_{t=1}^{T_n}(r_t^n+\gamma V^{\pi_{\theta}}(s_{t+1}^{n})-V^{\pi_{\theta}}(s_{t}^{n}))\nabla\log p_{\theta}(a_t^n|s_t^n)
$$

​	更进一步，我们可以将Actor网络和Value网络共用同一网络，最后一层分散即可。

### GAE优势函数

​	在A2C和AC节中，我们提到了优势函数：
$$
A^1_{\theta}(s_t,a_t)=r_t+\gamma V_\theta(s_{t+1})-V_\theta(s_{t})
$$
​	我们是将Q值转化为了r+V的形式，这里，我们可以像TD一样将异步拉长:
$$
A^2_{\theta}(s_t,a_t)=r_t+\gamma r_{t+1}+\gamma^{2}V_\theta(s_{t+2})-V_\theta(s_{t})
$$

$$
A^3_{\theta}(s_t,a_t)=r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^3V_\theta(s_{t+3})-V_\theta(s_{t})
$$

$$
A^T_{\theta}(s_t,a_t)=r_t+\gamma r_{t+1}+\gamma^{2}r_{t+2}+\gamma^3r_{t+3}+···+\gamma^Tr_{T}-V_\theta(s_{t})
$$

​	为了简化我们的式子，我们定义：
$$
\delta^V_t = r_t +\gamma V_\theta(s_{t+1})-V_\theta(s_t)
$$
​	则：
$$
A_\theta^1(s_t,a_t) = \delta^V_t
$$

$$
A_\theta^2(s_t,a_t) = \delta^V_t+\gamma\delta^V_{t+1}
$$

$$
A_\theta^3(s_t,a_t) = \delta^V_t+\gamma\delta^V_{t+1}+\gamma^2\delta^V_{t+2}
$$

​	当然，异步越多，效果偏差会小，所以我们将异步拉长到整个回合。

​	对于一个策略，位置处在前面的对整体效果影响当然更大，所以，我们给优势函数进行权重衰减，得到GAE优势函数：
$$
\begin{aligned}
A^{GAE}_\theta(s_t,a_t)&=(1-\lambda)(A^1_\theta+\lambda A^2_\theta+\lambda^2A^3_{\theta}+···)\newline&=(1-\lambda)(\delta^V_t+\lambda(\delta^V_t+\gamma\delta^V_{t+1})+···)\newline
&=(1-\lambda)(\delta^V_t(1+\lambda+\lambda^2+···)+\lambda\delta^V_{t+1}(\lambda+\lambda^2+···)+···)\newline&=\Sigma_{b=0}^{\infty}(\gamma\lambda)^b\delta^V_{t+b}
\end{aligned}
$$

​	于是我们的梯度函数可以写为：
$$
\nabla R(\tau)=\frac{1}{N}\Sigma_{n=1}^{N}\Sigma_{t=1}^{T_n}A^{GAE}_\theta(s_n^t,a_n^t)\nabla\log p_{\theta}(a_t^n|s_t^n)
$$



### PPO算法

​	基于策略梯度的算法有几个明显的问题——由于智能体需要拿到一整条策略$\pi_\theta$才能对策略网络进行更新，在训练的大部分时间，智能体都在与环境进行交互，只在完成一回合游戏之后才会更新一次网络，训练成本极大，收敛效果缓慢；第二个问题是，如果对于当下环境来说，智能体的动作是最佳的，但由于采集的策略不佳，它就会被负优化。

​	如何优化训练效率和精准训练呢？在DQN中的经验回放时一个很好的方法，也就是off-policy的更新方式可以很好的避免这种情况。我们如何将策略梯度转为off-policy呢？

#### 重要性采样（IS）

​	我们假设一种情景：在班级中，小明在做某件事得到了老师的表扬或批评，那么老师的这种对于小明动作的反馈可以更新你自己的动作。但是如果小明和你动作分布差距过大，比如，小明成绩差，那老师对小明的玩乐就是批评，你的成绩好，那老师就会鼓励你劳逸结合。

​	回到算法，为了将on-policy转为off-policy，我们需要一个旧的策略网络和在线更新的策略网络，旧的策略网络$\pi_{\theta'}$即小明,新的策略网络即自己。自己产生该动作与小明产生该动作的比例决定了该条经验对我们自己的更新重要性。下面我们证明这个重要性权重$\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta'}(a_t|s_t)}$的可行性：

​	on-policy不能经验回放的根本原因就是需要知道当前动作来判定奖励，就像Sarsa算法，根据实际采取的动作奖励来更新，我们对奖励期望进行如下变换：
$$
E_{x\sim p}[f(x)] = \int f(x)p(x)dx = \int f(x)\frac{p(x)}{q(x)}q(x)dx = E_{x\sim q}[f(x)\frac{p(x)}{q(x)}]
$$
​	

​	p不方便采样，但我们可以通过q去采样。在on-policy中PG公式为：
$$
\nabla R(\tau) = E_{(s_t,a_t)\sim \pi_{\theta}}[A^{GAE}(s_t,a_t)\nabla \log p_\theta(a^n_t|s^n_t)]
$$
​	代入交换公式：
$$
\nabla R(\tau) = E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(s_t,a_t)}{p_{\theta'}(s_t,a_t)}A^{GAE}(s_t,a_t)\nabla \log p_\theta(a^n_t|s^n_t)]
$$
​	注意到，每个环境出现的概率与策略几乎无关：
$$
\frac{p_\theta(s_t,a_t)}{p_{\theta'}(s_t,a_t)} =\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}\frac{p_\theta(s_t)}{p_{\theta'}(s_t)}\approx\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}==ratio
$$
​	即可得到off-policy公式：
$$
\nabla R(\tau) = E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{GAE}(s_t,a_t)\nabla \log p_\theta(a^n_t|s^n_t)]
$$
​	这里，我们利用
$$
\nabla f(x)=f(x)\nabla\log f(x)
$$
​	将$p_\theta(a_t|s_t)\nabla \log p_\theta(a^n_t|s^n_t)$化为$\nabla p_\theta(a_t|s_t)$:
$$
\begin{aligned}
\nabla J(\theta) &=E_{(s_t,a_t)\sim \pi_{\theta'}}[\frac{p_\theta(a_t|s_t)}{p_{\theta'}(a_t|s_t)}A^{GAE}(s_t,a_t)]\\&=E_{(s_t,a_t)\sim \pi_{\theta'}}[ratio*A^{GAE}(s_t,a_t)]
\end{aligned}
$$


​	这个公式反映出，我们可以将基于$\theta'$的策略存储在经验池，在使用$\theta$进行对经验池的$(s_t,a_t,r_t,v_t)$的概率估计，在更新$\theta$时计算优势A，通过$\theta$计算当下概率反向传播更新。

#### 更新修正

​	在上小节最后给到的公式中，我们注意到，如果两个策略相差过大，会导致$ratio$过大或者过小，PPO论文中给出了两种修正方式，一种是利用KL散度来确保两个分布相近，另一种是利用clip函数对更新进行截断，这里，我使用的是第二种，即clip截断。

![10](src\10.png)

​	PPO-Clip直接在目标损失中限制，保证新的参数和旧的参数的差距不会太大。也就是说，clip将更新限制在了$(1-\epsilon,1+\epsilon)$中，不会发生一次更新过大的情况。

![11](src\11.png)

​	待更新公式即：
$$
L^{Clip}(\theta) =E[min(ratio_t(\theta),clip(ratio_t(\theta),1-\epsilon,1+\epsilon))A_t]
$$

#### 多智能体

​	由于PPO实现了off-policy，可以在经验池中采样，故为了提高训练效率，PPO论文中提到了多智能体，将一个网络参数传给若干个智能体，分给多个GPU并行采样，更新时从这几个经验池中进行采样，加大训练密度。在本次训练中，限于本机GPU有问题，只使用了单智能体。

Open-AI给出的PPO论文中给出的关于clip的伪代码：

![13](src\13.png)

由于时间原因，PPO没有训练。

### DDPG算法

​	对于连续的动作，我们应如何得到Q函数进行Q-learning呢？

​	我们可以将state和action打包成一个状态-动作对，传入Critic中，得到Q值，类似于前面所提到的A-C算法。

​	值得注意的是，这个实际上和A-C、PG算法不太一样，DDPG的Actor网络输出的并不是概率，而是一个具体的动作，及获得一个状态中最大Q值的动作，Critic输出的Q值类似于DQN网络、Q表，所以，这里是可以使用经验回放，不需要走完一条策略的，也不使用权重梯度更新，前面提到的目标网络可以添加在这里以稳定效果。形象的说，这里的actor就像agent，critic就像Q表。

​	综上，在DDPG算法中，我们实际需要四个网络，即：actor, critic, Actor_target, cirtic_target。

​	由于没有找到连续环境加连续动作的环境，所以该算法并没有实践。

## 题外话

### MPC算法

​	本质上来说，MPC并不属于RL范畴，但它依旧可以完成RL的任务

![14](src\14.png)

​	一直以来，Yann LeCun 都是强化学习的批评者。他认为，强化学习这种方法需要大量的试验，非常低效。这和人类的学习方式大相径庭 —— 婴儿不是通过观察一百万个相同物体的样本来识别物体，或者尝试危险的东西并从中学习，而是通过观察、预测和与它们互动，即使没有监督。

​	在本次学习中，我试着使用了最简单的MPC，即让智能体进行若干次预行动，返回reward，选择最大reward的一条动作串完成其中第一个动作。

​	形式化来说，假设有一辆汽车正位于公路上行驶，我们希望它能沿着图中黄线做直线行驶（记住这是我们的目标），此时我们可以通过旋转方向盘来实现汽车位置的移动。开过车的人都知道，若方向盘打到底，会使得车身较快的进行偏转，而方向盘微微转动会使得车身的转向较缓。

![15](src\15.png)

​	接下来，我们将自己想象成机器人，想象如果自己只在固定间隔的时刻打方向盘，比如说开始时刻，第1秒，第2秒，第3秒......也就是每隔1秒调整一次车的方向，间隔之间的时间方向盘保持上一时刻点的状态。以下为示意图，其中箭头向右表示方向盘向右打，向左表示方向盘向左打，箭头与竖直方向偏离程度越大，则方向盘打的越狠。

​	然后，我们在头脑中想象两种的情景，第一个情景我们想玩漂移，于是将方向盘打的较狠，但最终还是需要达到我们的目标：汽车沿着黄线行驶。第二个情景我们吃了饭，因此车身不宜剧烈转动，于是我们轻打方向盘，最终达到目标。

​	两种方式的位置如下：

![16](src\16.png)	

​	我们根据两种情况返回的reward选择一个动作序列，完成第一个动作。之后，若使用我们最开始在头脑中设想的方向盘在0,1,2,3,4时刻转动的方案就很不合了，因为你可能会与你的目标越差越大。因此，我们要在头脑中重新想象几种情景，也就是在1,2,3,4,5时刻方向盘的转动对车身方向的影响，得到新的reward表。

​	优化方式可以选择加一些约束条件，比如KKT等等。

### Problem

​	在策略梯度网络中，由于梯度函数并不是单峰的，在learning-rate很小时，模型若一开始没有学习到好的策略，很容易陷入局部最优策略甚至学习不到策略，目前尚未找到好的改进方式。当然，一种显而易见的最优策略即agent始终与球同一水平面。







