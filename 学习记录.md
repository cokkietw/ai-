# 23-CS-谭博涵

## Gym-Taxi游戏基础模型

[model_1](model_1.py)

### 环境构建

​	我使用gym库自带的Taxi-v3作为基础环境。

​	考虑到这个状态空间包括了出租车的位置(有25种可能性,因为环境假设了一个5x5的网格世界)、乘客的位置(有4种可能性,分别对应Red、Green、Yellow和Blue四个位置)、目的地(也有4种可能性,同样对应四个指定位置)，100个状态是乘客的位置刚好是目的地的位置,这表示这一轮游戏的结束。故环境的state有500个，而每次操作有上，下，左，右，接乘客，放乘客6种操作，故action有6个。Q表设置为**500*6**大小。

### RL算法的选择

​	**Q-learning：**使用下一状态最优值来更新当前状态Q值：
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*\max(Q(s',a'))-Q(s,a))
$$
​		其中，$\alpha$为学习率，$R$为reward，$\gamma$为折扣因子。

​	**Sarsa：**使用下一状态选择的动作来更新当前状态Q值。
$$
Q(s,a)=Q(s,a)+\alpha*(R+\gamma*Q(s',a')-Q(s,a))
$$
​	二者区别：在Sarsa算法中，Q值的更新取决于当前状态采取的动作和新状态下选择的下一个动作，照顾到每一个动作；在Q-learning算法中，Q值的更新取决于当前状态下采取的动作，而下一动作是基于最大Q值选取的，更加冒险。这意味着Sarsa算法更考虑**当前动作**，Q-learning算法更考虑**最大reward**。

​	所以，Sarsa训练的模型更加稳定，Q-learning算法训练的模型回报更加可观。

### 训练和测试

​	二者的行走方式一致，区别在于train需要learn来更新Q表，test只需根据Q表来选择行走方式。

## Gym-Taxi游戏参数改进

### train_episode的选择

​	在Gym-Taxi环境中，由于离散情况不多，故经过适当的训练后，即可达到预期效果，若train_episode过大，大多次训练进行的是重复操作，训练成本过大，若train_episode过小，则无法对某些情况作出合理的判断选择，不能将更加正确的抉择的Q值凸显出来。

​	在本次训练中，我选择的train_episode为5000。

![](.\level-1\train.png)

![test](.\level-1\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.3$，train_episode=5000，test_episode=5000.

### $\epsilon$的选择

​	$\epsilon$表示智能体在选择action时，随机选择一个动作的概率，当其过高时，智能体不能很好的运用经验回放进行操作训练，当其过低时，不能完成多情况选择时的探索。

​	在本次训练中，我选择$\epsilon=0.01$.

![train](.\level-2\train.png)

![train](.\level-2\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.

### learning-rate的选择

​	learning-rate（以下简记为lr）作为学习率，指动作后的reward对上一状态Q值的更新权重。

​	适当的lr可以使模型快速收敛到合适的reward，过高的lr会使模型在reward谷底来回振荡；过低的lr会使训练成本——训练回合增大，也会使模型在局部最优点无法跳出。

​	在本次训练中，我选择lr=0.1。

​	测试结果同上一小节。

### $\gamma$的选择

​	$\gamma$即对未来reward的衰减值。$\gamma$越接近1，机器对未来的reward越敏感。我们可以用股票的方式来理解，10年后股票的涨跌对当下的买进和买入其实影响不大。我们引入衰减率即可表示出这种影响：
$$
\Delta reward=\Sigma_{k=0}^{n}\gamma^{k}reward_k
$$
​	在本次训练中，n=1，即只考虑当前和下一状态（时序差分）。

​	$\gamma$过高，则较远未来影响衰减体现不出来，$\gamma$过低，则较近未来影响体现不出来。

​	在本次训练中，我选择$\gamma=0.97$。

​	测试结果同上上小节。

## Gym-Taxi游戏环境改进

​	由于自己能力有限，对gym自带的taxi环境难以操作更改，故自己创建了一个taxi环境：[model_2](model_2.py).

![train](.\level-3\train.png)

![test](.\level-3\test.png)

### 更新reward表

​	首先,对于gym自带的taxi环境的reward,只有在街道乘客和正确放下乘客才能得到奖励,在此之前,智能体更多的是效率低下的随机探索。即像动态规划一样，在探索到最终状态才能逐步往前迭代，不能很好的利用整个环境。

​	由此，我学习了Reward Shaping技巧，使用曼哈顿距离作为Reward Shaping公式，在接取乘客之前，以距乘客的曼哈顿距离构造梯度reward表，在接取乘客之后，以距目的地的曼哈顿距离构造reward表，从而使智能体更好的与环境交互。

​	具体修改如下：**撞墙、错误接放：**-50；**向目标前进：**10-曼哈顿距离；**向目标远离：**曼哈顿距离-11；**正确接放**：20。

​	按以上处理后，发现智能体会在5-2和5-3反复行动，认为是仅操作终点的曼哈顿距离不能很好的体现非下一步墙体的reward。注意到，当终点为1-1或5-1时，智能体必经过3-1，而到3-1不会出现导致反复行动，同理，当终点是1-5或5-4时，必（假设）经过3-4。

​	所以我们加入优化，将3-1和3-4分别作为中间点，运用曼哈顿分步奖励，让智能体分步行动。

​	训练效果如下：

![train](.\level-4\train.png)

![test](.\level-4\test.png)

​	其中$\gamma=0.97$，learning_rate=0.1，$\epsilon=0.01$，train_episode=5000，test_episode=5000.	

​	考虑到reward shaping所设置的reward普遍较原reward较大，进行适当缩放，我们可以看到，利用reward shaping技巧可以很大程度减小前期的探索步骤，收敛过程亦相当可观。

​	值得注意的是，利用reward shaping test时可以很好的处理未进行train的情况，即可以更好的实时交互。

​	需要注意的是，对于当下Gym-Taxi游戏，我们是可以直接手动完成路线规划的，即设置大量的中间点，会使路线更加的美观和优秀甚至统一，但这样并不适用于大型离散或连续型的环境，也体现不了强化学习与环境的交互。

​	

## Gym-Pong游戏

### DQN神经网络构建

​	对比Gym-Taxi的离散情况，Gym-Pong环境是RGB图像，属于半连续状态，不能像Gym-Taxi一样构建Q-table进行更新。但注意到Q-table存在的意义是指导agent的行动，给出每个行动的reward，进而做出决策。对于连续情况，我们需要的本质是寻找一个东西去指导agent做选择，即神经网络。

​	神经网络可以将大数据处理成向量形式（在Pong游戏中为图像），在经过 神经网络后输出对应的reward，用以指导agent的action。同时我们可以像更新Q-table一样更新神经网络的参数，将神经网络训练成类似Q-table的函数。

​	在Pong环境中，我将一帧图片处理成shape=80*80的像素阵，压平成size=6400的向量，传入3层神经网络，得到size=6的rewards。

### 经验回放

​	对于Q-table，每个状态都有一个独立的空间存储reward，在除更新时一般情况不用考虑其他状态影响它的reward，但对于连续情况，所有状态共享一个神经网络，其他状态对神经网络的更新会影响到之前更新的权重。

​	在训练过程中，同一小段时间的图像大致一样，会导致神经网络朝着这个图像大幅更新，抵消之前的网络参数更新。对于这种时序上引起的反向更新，我们引入**经验回放**，即将过往状态存入一个经验池，在未来更新中不断采用先前的经验更新，同时，随着网络的更新，经验池溢出时也会使经验沿着时序更新，让网络总体上趋于最优。

​	对于一个经验池，我们使用队列构造（利用其先进先出的优势），每一次更新，我们从经验池中随机选取若干个经验用于更新网络。

### 网络更新优化

​	对于Q-table，我们选取下一Q_max_value时一般情况下也是无需考虑状态之间的影响，但对于复杂的DQN网络，状态数过多，神经网络评估的下一状态的值时很不稳定的，一个参数的更改可能会导致对其他状态的评估改变较大，比如在之前是-5，网络更新一次后变成12，就会导致我们对当前状态的更新出现误差，学习过程上下波动。

​	为了避免出现这种波动的情况，我们可以定义一个target_net，其是一个3000步不变的网络，用以评估下一状态的价值。

​	于是我们有了两个网络，一个是学习网络，用以计算当前状态的价值，不断更新；另一个是目标网络，每隔3000步更新（使用学习网络更新），用以评估下一状态的价值。

### 图像处理

​	游戏环境是一个RGB图像，但对于智能体而言，颜色的区分在gym-Pong中更多是阻碍其识别，我们将图像的有效区域切分后转为二值图像，即球和版为黑，其余部分为白。

### 卷积神经网络和池化

​	卷积神经网络是利用卷积核提取图像特征的一种方式，可以放大所需特征，对于DQN大型网络参数优化有明显效应。

​	在对图像进行卷积之后，加入池化层，以减少特征图的尺寸并保留重要特征。

DQN代码见[DQN](DQN_net.py).

### 策略梯度算法

​	在DQN神经网络中，在选择action时，对于网络输出的actions我们几乎是直接选取了价值最大的action，哪怕一个动作只是比另一个动作预期稍大。这样的选择会使动作僵化，导致那些潜在预期比较大的动作被采样的频率不高，更新速度较慢或是难以达到预期，在使用前面的方法训练后，最优效果大致在21：6，不能很好的收敛。

​	若我们选取action时，是使用rewards值所构成的归一化概率来选取action，则很好的解决了这个问题，即使用softmax函数对最后的rewards处理，它的输出即采取每个动作的概率，显然它是渐变的。

​	更新策略梯度网络时，我们使用对数概率，利用奖励取加权平均，即：
$$
loss=-\frac{1}{n}\Sigma reward_i\lg p_i
$$
​	最大化奖励即最小化loss，我们将loss反向传播即可训练策略梯度网络。

### 奖励衰减机制

​	在前文$\gamma$的选择中，我们提到了如下公式：
$$
\Delta reward=\Sigma_{k=0}^{n}\gamma^{k}reward_k
$$
​	在使用策略梯度算法时，一般是传入连续的多帧图像（本次训练以一局游戏为一组），这些奖励是连续的，不像经验池在短时间内只能得到下一状态的reward，我们就可以使用多步的奖励衰减去更新reward，将正负奖励延申，减少不必要的随机探索，使网络更快的收敛。

策略梯度代码见[p_net](p_net.py).

### 演员-评论家算法。

​	在使用DQN网络时，我们基于reward进行选择动作，弊端已经有所提及。而策略梯度网络也有其对应的弊端，它是通过累计奖励来更新网络的，在得到终止状态，才拿到最终奖励，很显然，当最终状态很远时，我们对策略的估计难免有所偏差。

​	通俗来说，DQN网络是步步更新，对每一个状态反向更新，设立一个个中间目标，更注重每个状态；而策略网络是对整个网络一次更新，更注重整体优化。

​	于是，我将DQN网络转化为状态的价值估计网络加入策略梯度算法中，这样既可以基于概率选择，整体统一优化，又保证了局部reward精确，估计结果偏差稳定。













